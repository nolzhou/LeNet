#IR entry      : @254_129_1_construct_wrapper.905
#attrs         :
training : 1
check_set_strategy_valid_once_only : 1
#Total params  : 20

%para1_inputs0 : <Tensor[Float32], (32, 1, 32, 32)>
%para2_inputs1 : <Tensor[Int32], (32)>
%para3_conv1.weight : <Ref[Tensor(F32)], (6, 1, 5, 5)>
%para4_conv2.weight : <Ref[Tensor(F32)], (16, 6, 5, 5)>
%para5_fc1.weight : <Ref[Tensor(F32)], (120, 400)>
%para6_fc1.bias : <Ref[Tensor(F32)], (120)>
%para7_fc2.weight : <Ref[Tensor(F32)], (84, 120)>
%para8_fc2.bias : <Ref[Tensor(F32)], (84)>
%para9_fc3.weight : <Ref[Tensor(F32)], (10, 84)>
%para10_fc3.bias : <Ref[Tensor(F32)], (10)>
%para11_moments.conv1.weight : <Ref[Tensor(F32)], (6, 1, 5, 5)>
%para12_moments.conv2.weight : <Ref[Tensor(F32)], (16, 6, 5, 5)>
%para13_moments.fc1.weight : <Ref[Tensor(F32)], (120, 400)>
%para14_moments.fc1.bias : <Ref[Tensor(F32)], (120)>
%para15_moments.fc2.weight : <Ref[Tensor(F32)], (84, 120)>
%para16_moments.fc2.bias : <Ref[Tensor(F32)], (84)>
%para17_moments.fc3.weight : <Ref[Tensor(F32)], (10, 84)>
%para18_moments.fc3.bias : <Ref[Tensor(F32)], (10)>
%para19_momentum : <Ref[Tensor(F32)], ()>
%para20_learning_rate : <Ref[Tensor(F32)], ()>

#Total subgraph : 1

subgraph attr:
training : 1
check_set_strategy_valid_once_only : 1
subgraph @254_129_1_construct_wrapper.905() {
  %0(equiv[CNode]843) = Load(%para3_conv1.weight, U)
      : (<Ref[Tensor(F32)], (6, 1, 5, 5)>, <UMonad>) -> (<Tensor[Float32], (6, 1, 5, 5)>)
  %1(equivoutput) = Conv2D(%para1_inputs0, %0) {instance name: conv2d} primitive_attrs: {kernel_size: (5, 5), mode: 1, out_channel: 6, input_names: [x, w], pad: (0, 0, 0, 0), pad_mode: 2, format: NCHW, pad_list: (0, 0, 0, 0), groups: 1, stride: (1, 1, 1, 1), group: 1, dilation: (1, 1, 1, 1), output_names: [output]}
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Float32], (6, 1, 5, 5)>) -> (<Tensor[Float32], (32, 6, 28, 28)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py(266)/        output = self.conv2d(x, self.weight)/
  %2(equiv[CNode]148) = ReLU(%1) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]}
      : (<Tensor[Float32], (32, 6, 28, 28)>) -> (<Tensor[Float32], (32, 6, 28, 28)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py(295)/        return self.relu(x)/
  %3(equivout) = MaxPool(%2) {instance name: max_pool} primitive_attrs: {pad_mode: 2, output_names: [output], kernel_size: (1, 1, 2, 2), format: NCHW, strides: (1, 1, 2, 2), input_names: [x]}
      : (<Tensor[Float32], (32, 6, 28, 28)>) -> (<Tensor[Float32], (32, 6, 14, 14)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py(142)/        out = self.max_pool(x)/
  %4(equiv[CNode]840) = Load(%para4_conv2.weight, U)
      : (<Ref[Tensor(F32)], (16, 6, 5, 5)>, <UMonad>) -> (<Tensor[Float32], (16, 6, 5, 5)>)
  %5(equivoutput) = Conv2D(%3, %4) {instance name: conv2d} primitive_attrs: {kernel_size: (5, 5), mode: 1, out_channel: 16, input_names: [x, w], pad: (0, 0, 0, 0), pad_mode: 2, format: NCHW, pad_list: (0, 0, 0, 0), groups: 1, stride: (1, 1, 1, 1), group: 1, dilation: (1, 1, 1, 1), output_names: [output]}
      : (<Tensor[Float32], (32, 6, 14, 14)>, <Tensor[Float32], (16, 6, 5, 5)>) -> (<Tensor[Float32], (32, 16, 10, 10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py(266)/        output = self.conv2d(x, self.weight)/
  %6(equiv[CNode]148) = ReLU(%5) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]}
      : (<Tensor[Float32], (32, 16, 10, 10)>) -> (<Tensor[Float32], (32, 16, 10, 10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py(295)/        return self.relu(x)/
  %7(equivout) = MaxPool(%6) {instance name: max_pool} primitive_attrs: {pad_mode: 2, output_names: [output], kernel_size: (1, 1, 2, 2), format: NCHW, strides: (1, 1, 2, 2), input_names: [x]}
      : (<Tensor[Float32], (32, 16, 10, 10)>) -> (<Tensor[Float32], (32, 16, 5, 5)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py(142)/        out = self.max_pool(x)/
  %8(equiv[CNode]251) = Reshape(%7, (32, -1)) primitive_attrs: {output_names: [output], input_names: [tensor, shape]}
      : (<Tensor[Float32], (32, 16, 5, 5)>, <Tuple[Int64*2], sequence_nodes={node={<freed node>}, node={ValueNode<ValueTuple> (32, -1), elements_use_flags: {ptr: 0x159961d2220, value: [const vector][1, 1]}}, node={<freed node>}, node={ValueNode<ValueTuple> (32, -1), elements_use_flags: {ptr: 0x159961d2220, value: [const vector][1, 1]}}, node={ValueNode<ValueTuple> (32, -1), elements_use_flags: {ptr: 0x159961d2220, value: [const vector][1, 1]}}}>) -> (<Tensor[Float32], (32, 400)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(215)/        return F.reshape(x, (F.shape(x)[0], -1))/
  %9(equiv[CNode]834) = Load(%para5_fc1.weight, U)
      : (<Ref[Tensor(F32)], (120, 400)>, <UMonad>) -> (<Tensor[Float32], (120, 400)>)
  %10(equivx) = MatMul(%8, %9) {instance name: matmul} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: true, transpose_x1: false, transpose_b: true}
      : (<Tensor[Float32], (32, 400)>, <Tensor[Float32], (120, 400)>) -> (<Tensor[Float32], (32, 120)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(324)/        x = self.matmul(x, self.weight)/
  %11(equiv[CNode]835) = Load(%para6_fc1.bias, U)
      : (<Ref[Tensor(F32)], (120)>, <UMonad>) -> (<Tensor[Float32], (120)>)
  %12(equivx) = BiasAdd(%10, %11) {instance name: bias_add} primitive_attrs: {output_names: [output], format: NCHW, input_names: [x, b]}
      : (<Tensor[Float32], (32, 120)>, <Tensor[Float32], (120)>) -> (<Tensor[Float32], (32, 120)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(326)/            x = self.bias_add(x, self.bias)/
  %13(equiv[CNode]148) = ReLU(%12) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]}
      : (<Tensor[Float32], (32, 120)>) -> (<Tensor[Float32], (32, 120)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py(295)/        return self.relu(x)/
  %14(equiv[CNode]828) = Load(%para7_fc2.weight, U)
      : (<Ref[Tensor(F32)], (84, 120)>, <UMonad>) -> (<Tensor[Float32], (84, 120)>)
  %15(equivx) = MatMul(%13, %14) {instance name: matmul} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: true, transpose_x1: false, transpose_b: true}
      : (<Tensor[Float32], (32, 120)>, <Tensor[Float32], (84, 120)>) -> (<Tensor[Float32], (32, 84)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(324)/        x = self.matmul(x, self.weight)/
  %16(equiv[CNode]829) = Load(%para8_fc2.bias, U)
      : (<Ref[Tensor(F32)], (84)>, <UMonad>) -> (<Tensor[Float32], (84)>)
  %17(equivx) = BiasAdd(%15, %16) {instance name: bias_add} primitive_attrs: {output_names: [output], format: NCHW, input_names: [x, b]}
      : (<Tensor[Float32], (32, 84)>, <Tensor[Float32], (84)>) -> (<Tensor[Float32], (32, 84)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(326)/            x = self.bias_add(x, self.bias)/
  %18(equiv[CNode]148) = ReLU(%17) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]}
      : (<Tensor[Float32], (32, 84)>) -> (<Tensor[Float32], (32, 84)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py(295)/        return self.relu(x)/
  %19(equiv[CNode]822) = Load(%para9_fc3.weight, U)
      : (<Ref[Tensor(F32)], (10, 84)>, <UMonad>) -> (<Tensor[Float32], (10, 84)>)
  %20(equivx) = MatMul(%18, %19) {instance name: matmul} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: true, transpose_x1: false, transpose_b: true}
      : (<Tensor[Float32], (32, 84)>, <Tensor[Float32], (10, 84)>) -> (<Tensor[Float32], (32, 10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(324)/        x = self.matmul(x, self.weight)/
  %21(equiv[CNode]823) = Load(%para10_fc3.bias, U)
      : (<Ref[Tensor(F32)], (10)>, <UMonad>) -> (<Tensor[Float32], (10)>)
  %22(equivx) = BiasAdd(%20, %21) {instance name: bias_add} primitive_attrs: {output_names: [output], format: NCHW, input_names: [x, b]}
      : (<Tensor[Float32], (32, 10)>, <Tensor[Float32], (10)>) -> (<Tensor[Float32], (32, 10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(326)/            x = self.bias_add(x, self.bias)/
  %23(equivx) = SparseSoftmaxCrossEntropyWithLogits(%22, %para2_inputs1) {instance name: sparse_softmax_cross_entropy} primitive_attrs: {output_names: [output], input_names: [features, labels], sens: 1.000000, is_grad: false}
      : (<Tensor[Float32], (32, 10)>, <Tensor[Int32], (32)>) -> (<Tensor[Float32], ()>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py(627)/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  %24(grad) = SparseSoftmaxCrossEntropyWithLogits(%22, %para2_inputs1) primitive_attrs: {output_names: [output], input_names: [features, labels], sens: 1.000000, is_grad: true} cnode_primal_attrs: {forward_node_name: SparseSoftmaxCrossEntropyWithLogits_7690}
      : (<Tensor[Float32], (32, 10)>, <Tensor[Int32], (32)>) -> (<Tensor[Float32], (32, 10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(794)/            grad = grad_op(logits, labels)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py(627)/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  %25(grad) = Depend(%24, %23) primitive_attrs: {side_effect_propagate: 1} cnode_primal_attrs: {forward_node_name: SparseSoftmaxCrossEntropyWithLogits_7690}
      : (<Tensor[Float32], (32, 10)>, <Tensor[Float32], ()>) -> (<Tensor[Float32], (32, 10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(795)/            grad = F.depend(grad, out)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py(627)/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  %26(dx) = MatMul(%25, %19) primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false} cnode_primal_attrs: {forward_node_name: MatMul_7787}
      : (<Tensor[Float32], (32, 10)>, <Tensor[Float32], (10, 84)>) -> (<Tensor[Float32], (32, 84)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_math_ops.py(191)/            dx = mul1(dout, w)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(324)/        x = self.matmul(x, self.weight)/
  %27(dx) = ReluGrad(%26, %18) primitive_attrs: {output_names: [output], input_names: [y_backprop, x]} cnode_primal_attrs: {forward_node_name: ReLU_7857}
      : (<Tensor[Float32], (32, 84)>, <Tensor[Float32], (32, 84)>) -> (<Tensor[Float32], (32, 84)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(434)/        dx = input_grad(dout, out)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py(295)/        return self.relu(x)/
  %28(dx) = MatMul(%27, %14) primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false} cnode_primal_attrs: {forward_node_name: MatMul_7921}
      : (<Tensor[Float32], (32, 84)>, <Tensor[Float32], (84, 120)>) -> (<Tensor[Float32], (32, 120)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_math_ops.py(191)/            dx = mul1(dout, w)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(324)/        x = self.matmul(x, self.weight)/
  %29(dx) = ReluGrad(%28, %13) primitive_attrs: {output_names: [output], input_names: [y_backprop, x]} cnode_primal_attrs: {forward_node_name: ReLU_7991}
      : (<Tensor[Float32], (32, 120)>, <Tensor[Float32], (32, 120)>) -> (<Tensor[Float32], (32, 120)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(434)/        dx = input_grad(dout, out)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py(295)/        return self.relu(x)/
  %30(dx) = MatMul(%29, %9) primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false} cnode_primal_attrs: {forward_node_name: MatMul_8055}
      : (<Tensor[Float32], (32, 120)>, <Tensor[Float32], (120, 400)>) -> (<Tensor[Float32], (32, 400)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_math_ops.py(191)/            dx = mul1(dout, w)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(324)/        x = self.matmul(x, self.weight)/
  %31([CNode]906) = Reshape(%30, (32, 16, 5, 5)) primitive_attrs: {output_names: [output], input_names: [tensor, shape]} cnode_primal_attrs: {forward_node_name: Reshape_8126}
      : (<Tensor[Float32], (32, 400)>, <Tuple[Int64*4]>) -> (<Tensor[Float32], (32, 16, 5, 5)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_array_ops.py(187)/        return reshape(dout, shapex), zeros_like(shp)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(215)/        return F.reshape(x, (F.shape(x)[0], -1))/
  %32(dx) = MaxPoolGrad(%6, %7, %31) primitive_attrs: {pad_mode: 2, output_names: [output], kernel_size: (1, 1, 2, 2), format: NCHW, strides: (1, 1, 2, 2), input_names: [x_origin, out_origin, grad]} cnode_primal_attrs: {forward_node_name: MaxPool_8137}
      : (<Tensor[Float32], (32, 16, 10, 10)>, <Tensor[Float32], (32, 16, 5, 5)>, <Tensor[Float32], (32, 16, 5, 5)>) -> (<Tensor[Float32], (32, 16, 10, 10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(251)/        dx = maxpool_grad(x, out, dout)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py(142)/        out = self.max_pool(x)/
  %33(dx) = ReluGrad(%32, %6) primitive_attrs: {output_names: [output], input_names: [y_backprop, x]} cnode_primal_attrs: {forward_node_name: ReLU_8149}
      : (<Tensor[Float32], (32, 16, 10, 10)>, <Tensor[Float32], (32, 16, 10, 10)>) -> (<Tensor[Float32], (32, 16, 10, 10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(434)/        dx = input_grad(dout, out)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py(295)/        return self.relu(x)/
  %34(dx) = Conv2DBackpropInput(%33, %4, (32, 6, 14, 14)) primitive_attrs: {kernel_size: (5, 5), mode: 1, out_channel: 16, input_names: [out_backprop, filter, input_sizes], pad: (0, 0, 0, 0), pad_mode: 2, format: NCHW, pad_list: (0, 0, 0, 0), groups: 1, stride: (1, 1, 1, 1), group: 1, dilation: (1, 1, 1, 1), output_names: [output]} cnode_primal_attrs: {forward_node_name: Conv2D_8172}
      : (<Tensor[Float32], (32, 16, 10, 10)>, <Tensor[Float32], (16, 6, 5, 5)>, <Tuple[Int64*4]>) -> (<Tensor[Float32], (32, 6, 14, 14)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(64)/        dx = input_grad(dout, w, x_shape)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py(266)/        output = self.conv2d(x, self.weight)/
  %35(dx) = MaxPoolGrad(%2, %3, %34) primitive_attrs: {pad_mode: 2, output_names: [output], kernel_size: (1, 1, 2, 2), format: NCHW, strides: (1, 1, 2, 2), input_names: [x_origin, out_origin, grad]} cnode_primal_attrs: {forward_node_name: MaxPool_8190}
      : (<Tensor[Float32], (32, 6, 28, 28)>, <Tensor[Float32], (32, 6, 14, 14)>, <Tensor[Float32], (32, 6, 14, 14)>) -> (<Tensor[Float32], (32, 6, 28, 28)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(251)/        dx = maxpool_grad(x, out, dout)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py(142)/        out = self.max_pool(x)/
  %36(dx) = ReluGrad(%35, %2) primitive_attrs: {output_names: [output], input_names: [y_backprop, x]} cnode_primal_attrs: {forward_node_name: ReLU_8202}
      : (<Tensor[Float32], (32, 6, 28, 28)>, <Tensor[Float32], (32, 6, 28, 28)>) -> (<Tensor[Float32], (32, 6, 28, 28)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(434)/        dx = input_grad(dout, out)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py(295)/        return self.relu(x)/
  %37(dw) = Conv2DBackpropFilter(%36, %para1_inputs0, (6, 1, 5, 5)) primitive_attrs: {kernel_size: (5, 5), mode: 1, out_channel: 6, input_names: [out_backprop, input, filter_sizes], pad: (0, 0, 0, 0), pad_mode: 2, format: NCHW, pad_list: (0, 0, 0, 0), groups: 1, stride: (1, 1), group: 1, dilation: (1, 1, 1, 1), output_names: [output]} cnode_primal_attrs: {forward_node_name: Conv2D_8225}
      : (<Tensor[Float32], (32, 6, 28, 28)>, <Tensor[Float32], (32, 1, 32, 32)>, <Tuple[Int64*4]>) -> (<Tensor[Float32], (6, 1, 5, 5)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(66)/        dw = filter_grad(dout, x, w_shape)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py(266)/        output = self.conv2d(x, self.weight)/
  %38([CNode]907) = MakeTuple(%21, %16, %11, %4, %0, %9, %14, %19)
      : (<Tensor[Float32], (10)>, <Tensor[Float32], (84)>, <Tensor[Float32], (120)>, <Tensor[Float32], (16, 6, 5, 5)>, <Tensor[Float32], (6, 1, 5, 5)>, <Tensor[Float32], (120, 400)>, <Tensor[Float32], (84, 120)>, <Tensor[Float32], (10, 84)>) -> (<Tuple[Tensor[Float32]*8], sequence_nodes={node={254_129_1_construct_wrapper.905:[CNode]907{[0]: ValueNode<Primitive> MakeTuple, [1]: equiv[CNode]823, [2]: equiv[CNode]829, [3]: equiv[CNode]835, [4]: equiv[CNode]840, [5]: equiv[CNode]843, [6]: equiv[CNode]834, [7]: equiv[CNode]828, [8]: equiv[CNode]822}, elements_use_flags: {ptr: 0x1599709d4c0, value: [const vector][1, 1, 1, 1, 1, 1, 1, 1]}}}>)
  %39([CNode]908) = UpdateState(U, %38)
      : (<UMonad>, <Tuple[Tensor[Float32]*8], sequence_nodes={node={254_129_1_construct_wrapper.905:[CNode]907{[0]: ValueNode<Primitive> MakeTuple, [1]: equiv[CNode]823, [2]: equiv[CNode]829, [3]: equiv[CNode]835, [4]: equiv[CNode]840, [5]: equiv[CNode]843, [6]: equiv[CNode]834, [7]: equiv[CNode]828, [8]: equiv[CNode]822}, elements_use_flags: {ptr: 0x1599709d4c0, value: [const vector][1, 1, 1, 1, 1, 1, 1, 1]}}}>) -> (<UMonad>)
  %40([CNode]909) = BiasAddGrad(%25) primitive_attrs: {output_names: [output], format: NCHW, input_names: [dout]} cnode_primal_attrs: {forward_node_name: BiasAdd_7785}
      : (<Tensor[Float32], (32, 10)>) -> (<Tensor[Float32], (10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(36)/        return dout, bias_grad(dout)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(326)/            x = self.bias_add(x, self.bias)/
  %41([CNode]602) = ApplyMomentum(%para10_fc3.bias, %para18_moments.fc3.bias, %para20_learning_rate, %40, %para19_momentum, %39) {instance name: opt} primitive_attrs: {output_names: [output], side_effect_mem: true, use_nesterov: false, input_names: [variable, accumulation, learning_rate, gradient, momentum], use_locking: false, gradient_scale: 1.000000}
      : (<Ref[Tensor(F32)], (10)>, <Ref[Tensor(F32)], (10)>, <Ref[Tensor(F32)], ()>, <Tensor[Float32], (10)>, <Ref[Tensor(F32)], ()>, <UMonad>) -> (<Tensor[Float32], (10)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %42([CNode]853) = UpdateState(%39, %41)
      : (<UMonad>, <Tensor[Float32], (10)>) -> (<UMonad>)
  %43(dw) = MatMul(%25, %18) primitive_attrs: {output_names: [output], transpose_a: true, input_names: [x1, x2], transpose_x2: false, transpose_x1: true, transpose_b: false} cnode_primal_attrs: {forward_node_name: MatMul_7787}
      : (<Tensor[Float32], (32, 10)>, <Tensor[Float32], (32, 84)>) -> (<Tensor[Float32], (10, 84)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_math_ops.py(193)/            dw = mul2(dout, x)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(324)/        x = self.matmul(x, self.weight)/
  %44([CNode]602) = ApplyMomentum(%para9_fc3.weight, %para17_moments.fc3.weight, %para20_learning_rate, %43, %para19_momentum, %42) {instance name: opt} primitive_attrs: {output_names: [output], side_effect_mem: true, use_nesterov: false, input_names: [variable, accumulation, learning_rate, gradient, momentum], use_locking: false, gradient_scale: 1.000000}
      : (<Ref[Tensor(F32)], (10, 84)>, <Ref[Tensor(F32)], (10, 84)>, <Ref[Tensor(F32)], ()>, <Tensor[Float32], (10, 84)>, <Ref[Tensor(F32)], ()>, <UMonad>) -> (<Tensor[Float32], (10, 84)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %45([CNode]855) = UpdateState(%42, %44)
      : (<UMonad>, <Tensor[Float32], (10, 84)>) -> (<UMonad>)
  %46([CNode]909) = BiasAddGrad(%27) primitive_attrs: {output_names: [output], format: NCHW, input_names: [dout]} cnode_primal_attrs: {forward_node_name: BiasAdd_7919}
      : (<Tensor[Float32], (32, 84)>) -> (<Tensor[Float32], (84)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(36)/        return dout, bias_grad(dout)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(326)/            x = self.bias_add(x, self.bias)/
  %47([CNode]602) = ApplyMomentum(%para8_fc2.bias, %para16_moments.fc2.bias, %para20_learning_rate, %46, %para19_momentum, %45) {instance name: opt} primitive_attrs: {output_names: [output], side_effect_mem: true, use_nesterov: false, input_names: [variable, accumulation, learning_rate, gradient, momentum], use_locking: false, gradient_scale: 1.000000}
      : (<Ref[Tensor(F32)], (84)>, <Ref[Tensor(F32)], (84)>, <Ref[Tensor(F32)], ()>, <Tensor[Float32], (84)>, <Ref[Tensor(F32)], ()>, <UMonad>) -> (<Tensor[Float32], (84)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %48([CNode]857) = UpdateState(%45, %47)
      : (<UMonad>, <Tensor[Float32], (84)>) -> (<UMonad>)
  %49(dw) = MatMul(%27, %13) primitive_attrs: {output_names: [output], transpose_a: true, input_names: [x1, x2], transpose_x2: false, transpose_x1: true, transpose_b: false} cnode_primal_attrs: {forward_node_name: MatMul_7921}
      : (<Tensor[Float32], (32, 84)>, <Tensor[Float32], (32, 120)>) -> (<Tensor[Float32], (84, 120)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_math_ops.py(193)/            dw = mul2(dout, x)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(324)/        x = self.matmul(x, self.weight)/
  %50([CNode]602) = ApplyMomentum(%para7_fc2.weight, %para15_moments.fc2.weight, %para20_learning_rate, %49, %para19_momentum, %48) {instance name: opt} primitive_attrs: {output_names: [output], side_effect_mem: true, use_nesterov: false, input_names: [variable, accumulation, learning_rate, gradient, momentum], use_locking: false, gradient_scale: 1.000000}
      : (<Ref[Tensor(F32)], (84, 120)>, <Ref[Tensor(F32)], (84, 120)>, <Ref[Tensor(F32)], ()>, <Tensor[Float32], (84, 120)>, <Ref[Tensor(F32)], ()>, <UMonad>) -> (<Tensor[Float32], (84, 120)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %51([CNode]859) = UpdateState(%48, %50)
      : (<UMonad>, <Tensor[Float32], (84, 120)>) -> (<UMonad>)
  %52([CNode]909) = BiasAddGrad(%29) primitive_attrs: {output_names: [output], format: NCHW, input_names: [dout]} cnode_primal_attrs: {forward_node_name: BiasAdd_8053}
      : (<Tensor[Float32], (32, 120)>) -> (<Tensor[Float32], (120)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(36)/        return dout, bias_grad(dout)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(326)/            x = self.bias_add(x, self.bias)/
  %53([CNode]602) = ApplyMomentum(%para6_fc1.bias, %para14_moments.fc1.bias, %para20_learning_rate, %52, %para19_momentum, %51) {instance name: opt} primitive_attrs: {output_names: [output], side_effect_mem: true, use_nesterov: false, input_names: [variable, accumulation, learning_rate, gradient, momentum], use_locking: false, gradient_scale: 1.000000}
      : (<Ref[Tensor(F32)], (120)>, <Ref[Tensor(F32)], (120)>, <Ref[Tensor(F32)], ()>, <Tensor[Float32], (120)>, <Ref[Tensor(F32)], ()>, <UMonad>) -> (<Tensor[Float32], (120)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %54([CNode]861) = UpdateState(%51, %53)
      : (<UMonad>, <Tensor[Float32], (120)>) -> (<UMonad>)
  %55(dw) = MatMul(%29, %8) primitive_attrs: {output_names: [output], transpose_a: true, input_names: [x1, x2], transpose_x2: false, transpose_x1: true, transpose_b: false} cnode_primal_attrs: {forward_node_name: MatMul_8055}
      : (<Tensor[Float32], (32, 120)>, <Tensor[Float32], (32, 400)>) -> (<Tensor[Float32], (120, 400)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_math_ops.py(193)/            dw = mul2(dout, x)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py(324)/        x = self.matmul(x, self.weight)/
  %56([CNode]602) = ApplyMomentum(%para5_fc1.weight, %para13_moments.fc1.weight, %para20_learning_rate, %55, %para19_momentum, %54) {instance name: opt} primitive_attrs: {output_names: [output], side_effect_mem: true, use_nesterov: false, input_names: [variable, accumulation, learning_rate, gradient, momentum], use_locking: false, gradient_scale: 1.000000}
      : (<Ref[Tensor(F32)], (120, 400)>, <Ref[Tensor(F32)], (120, 400)>, <Ref[Tensor(F32)], ()>, <Tensor[Float32], (120, 400)>, <Ref[Tensor(F32)], ()>, <UMonad>) -> (<Tensor[Float32], (120, 400)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %57([CNode]863) = UpdateState(%54, %56)
      : (<UMonad>, <Tensor[Float32], (120, 400)>) -> (<UMonad>)
  %58(dw) = Conv2DBackpropFilter(%33, %3, (16, 6, 5, 5)) primitive_attrs: {kernel_size: (5, 5), mode: 1, out_channel: 16, input_names: [out_backprop, input, filter_sizes], pad: (0, 0, 0, 0), pad_mode: 2, format: NCHW, pad_list: (0, 0, 0, 0), groups: 1, stride: (1, 1), group: 1, dilation: (1, 1, 1, 1), output_names: [output]} cnode_primal_attrs: {forward_node_name: Conv2D_8172}
      : (<Tensor[Float32], (32, 16, 10, 10)>, <Tensor[Float32], (32, 6, 14, 14)>, <Tuple[Int64*4]>) -> (<Tensor[Float32], (16, 6, 5, 5)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\ops\_grad\grad_nn_ops.py(66)/        dw = filter_grad(dout, x, w_shape)/
      # Corresponding forward node candidate:
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py(266)/        output = self.conv2d(x, self.weight)/
  %59([CNode]602) = ApplyMomentum(%para4_conv2.weight, %para12_moments.conv2.weight, %para20_learning_rate, %58, %para19_momentum, %57) {instance name: opt} primitive_attrs: {output_names: [output], side_effect_mem: true, use_nesterov: false, input_names: [variable, accumulation, learning_rate, gradient, momentum], use_locking: false, gradient_scale: 1.000000}
      : (<Ref[Tensor(F32)], (16, 6, 5, 5)>, <Ref[Tensor(F32)], (16, 6, 5, 5)>, <Ref[Tensor(F32)], ()>, <Tensor[Float32], (16, 6, 5, 5)>, <Ref[Tensor(F32)], ()>, <UMonad>) -> (<Tensor[Float32], (16, 6, 5, 5)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %60([CNode]865) = UpdateState(%57, %59)
      : (<UMonad>, <Tensor[Float32], (16, 6, 5, 5)>) -> (<UMonad>)
  %61([CNode]602) = ApplyMomentum(%para3_conv1.weight, %para11_moments.conv1.weight, %para20_learning_rate, %37, %para19_momentum, %60) {instance name: opt} primitive_attrs: {output_names: [output], side_effect_mem: true, use_nesterov: false, input_names: [variable, accumulation, learning_rate, gradient, momentum], use_locking: false, gradient_scale: 1.000000}
      : (<Ref[Tensor(F32)], (6, 1, 5, 5)>, <Ref[Tensor(F32)], (6, 1, 5, 5)>, <Ref[Tensor(F32)], ()>, <Tensor[Float32], (6, 1, 5, 5)>, <Ref[Tensor(F32)], ()>, <UMonad>) -> (<Tensor[Float32], (6, 1, 5, 5)>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %62(success) = Depend(true, %61) primitive_attrs: {side_effect_propagate: 1}
      : (<Bool>, <Tensor[Float32], (6, 1, 5, 5)>) -> (<Bool>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %63(success) = Depend(true, %59) primitive_attrs: {side_effect_propagate: 1}
      : (<Bool>, <Tensor[Float32], (16, 6, 5, 5)>) -> (<Bool>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %64(success) = Depend(true, %56) primitive_attrs: {side_effect_propagate: 1}
      : (<Bool>, <Tensor[Float32], (120, 400)>) -> (<Bool>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %65(success) = Depend(true, %53) primitive_attrs: {side_effect_propagate: 1}
      : (<Bool>, <Tensor[Float32], (120)>) -> (<Bool>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %66(success) = Depend(true, %50) primitive_attrs: {side_effect_propagate: 1}
      : (<Bool>, <Tensor[Float32], (84, 120)>) -> (<Bool>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %67(success) = Depend(true, %47) primitive_attrs: {side_effect_propagate: 1}
      : (<Bool>, <Tensor[Float32], (84)>) -> (<Bool>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %68(success) = Depend(true, %44) primitive_attrs: {side_effect_propagate: 1}
      : (<Bool>, <Tensor[Float32], (10, 84)>) -> (<Bool>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %69(success) = Depend(true, %41) primitive_attrs: {side_effect_propagate: 1}
      : (<Bool>, <Tensor[Float32], (10)>) -> (<Bool>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(37)/        success = F.depend(True, opt(weight, moment, learning_rate, gradient, momentum))/
  %70(success) = MakeTuple(%62, %63, %64, %65, %66, %67, %68, %69)
      : (<Bool>, <Bool>, <Bool>, <Bool>, <Bool>, <Bool>, <Bool>, <Bool>) -> (<Tuple[Bool*8], sequence_nodes={node={254_129_1_construct_wrapper.905:success{[0]: ValueNode<Primitive> MakeTuple, [1]: success, [2]: success, [3]: success, [4]: success, [5]: success, [6]: success, [7]: success, [8]: success}, elements_use_flags: {ptr: 0x159970a04c0, value: [const vector][1, 1, 1, 1, 1, 1, 1, 1]}}}>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\momentum.py(181)/            success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  %71(loss) = Depend(%23, %70) primitive_attrs: {side_effect_propagate: 1}
      : (<Tensor[Float32], ()>, <Tuple[Bool*8], sequence_nodes={node={254_129_1_construct_wrapper.905:success{[0]: ValueNode<Primitive> MakeTuple, [1]: success, [2]: success, [3]: success, [4]: success, [5]: success, [6]: success, [7]: success, [8]: success}, elements_use_flags: {ptr: 0x159970a04c0, value: [const vector][1, 1, 1, 1, 1, 1, 1, 1]}}}>) -> (<Tensor[Float32], ()>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py(365)/        loss = F.depend(loss, self.optimizer(grads))/
  %72([CNode]852) = UpdateState(%60, %61)
      : (<UMonad>, <Tensor[Float32], (6, 1, 5, 5)>) -> (<UMonad>)
  %73(loss) = Depend(%71, %72) primitive_attrs: {side_effect_propagate: 1}
      : (<Tensor[Float32], ()>, <UMonad>) -> (<Tensor[Float32], ()>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py(365)/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%73)
      : (<Tensor[Float32], ()>)
      # In file D:\Users\nol\anaconda3\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py(366)/        return loss/
}

